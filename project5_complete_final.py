# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12Dgd42oKnheadPE_bDA1Sr-8-2a6H8Y1
"""

from google.colab import drive
drive.mount('/content/drive')

"""## STEP 1: UNDERSTAND & EXPLORE THE DATASET"""

import os
import matplotlib.pyplot as plt
from PIL import Image

# ----------------------------------------
# CONFIGURATION (LOCKED ORDER)
# ----------------------------------------
DATASET_PATH = "/content/drive/MyDrive/data/Tumour"

SPLITS = ["train", "val", "test"]  # fixed order
CLASSES = ["glioma", "meningioma", "no_tumor", "pituitary"]  # fixed order

IMAGE_EXTENSIONS = (".png", ".jpg", ".jpeg")

# ----------------------------------------
# 1Ô∏è‚É£ COUNT IMAGES PER CLASS (ORDERED)
# ----------------------------------------
print("üìä Class Distribution (Order Locked):\n")

for split in SPLITS:
    split_path = os.path.join(DATASET_PATH, split)
    print(f"‚ñ∂ {split.upper()} SET")

    for cls in CLASSES:
        cls_path = os.path.join(split_path, cls)

        if not os.path.exists(cls_path):
            print(f"   {cls}: ‚ùå folder not found")
            continue

        images = [
            f for f in os.listdir(cls_path)
            if f.lower().endswith(IMAGE_EXTENSIONS)
        ]

        print(f"   {cls}: {len(images)} images")

    print()

# ----------------------------------------
# 2Ô∏è‚É£ VISUALIZE ONE IMAGE PER CLASS (TRAIN)
# ----------------------------------------
train_path = os.path.join(DATASET_PATH, "train")

plt.figure(figsize=(8, 8))
i = 1

for cls in CLASSES:
    cls_path = os.path.join(train_path, cls)

    if not os.path.exists(cls_path):
        continue

    image_files = [
        f for f in os.listdir(cls_path)
        if f.lower().endswith(IMAGE_EXTENSIONS)
    ]

    if not image_files:
        continue

    img = Image.open(os.path.join(cls_path, image_files[0]))

    plt.subplot(2, 2, i)
    plt.imshow(img)
    plt.title(cls)
    plt.axis("off")
    i += 1

plt.suptitle("Sample MRI Images (Training Set)", fontsize=14)
plt.tight_layout()
plt.show()

"""## STEP 2: DATASET LOADING"""

import tensorflow as tf
import os

# ----------------------------------------
# CONFIGURATION (MUST MATCH STEP-1)
# ----------------------------------------
DATASET_PATH = "/content/drive/MyDrive/data/Tumour"

IMG_SIZE = (224, 224)
BATCH_SIZE = 32

SPLITS = ["train", "val", "test"]
CLASS_NAMES = ["glioma", "meningioma", "no_tumor", "pituitary"]

# ----------------------------------------
# FUNCTION TO LOAD DATASET
# ----------------------------------------
def load_dataset(split):
    split_path = os.path.join(DATASET_PATH, split)

    dataset = tf.keras.utils.image_dataset_from_directory(
        split_path,
        labels="inferred",
        label_mode="categorical",
        class_names=CLASS_NAMES,   # üîí ORDER LOCKED
        image_size=IMG_SIZE,
        batch_size=BATCH_SIZE,
        shuffle=True
    )
    return dataset

# ----------------------------------------
# LOAD DATASETS
# ----------------------------------------
train_ds = load_dataset("train")
val_ds   = load_dataset("val")
test_ds  = load_dataset("test")

# ----------------------------------------
# VERIFY CLASS ORDER
# ----------------------------------------
print("Class order used by TensorFlow:")
print(train_ds.class_names)

"""## STEP 3: NORMALIZATION & DATA PIPELINE OPTIMIZATION"""

import tensorflow as tf

# ----------------------------------------
# NORMALIZATION LAYER
# ----------------------------------------
normalization_layer = tf.keras.layers.Rescaling(1.0 / 255)

# ----------------------------------------
# APPLY NORMALIZATION
# ----------------------------------------
train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
val_ds   = val_ds.map(lambda x, y: (normalization_layer(x), y))
test_ds  = test_ds.map(lambda x, y: (normalization_layer(x), y))

# ----------------------------------------
# PERFORMANCE OPTIMIZATION
# ----------------------------------------
AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds   = val_ds.cache().prefetch(buffer_size=AUTOTUNE)
test_ds  = test_ds.cache().prefetch(buffer_size=AUTOTUNE)

print("‚úÖ Step-3 completed: Normalization & pipeline optimization applied")

for images, labels in train_ds.take(1):
    print("Image min:", tf.reduce_min(images).numpy())
    print("Image max:", tf.reduce_max(images).numpy())

"""## STEP 4: DATA AUGMENTATION"""

import tensorflow as tf

# ----------------------------------------
# DATA AUGMENTATION LAYER
# ----------------------------------------
data_augmentation = tf.keras.Sequential(
    [
        tf.keras.layers.RandomFlip("horizontal"),
        tf.keras.layers.RandomRotation(0.1),
        tf.keras.layers.RandomZoom(0.1),
        tf.keras.layers.RandomContrast(0.1),
    ],
    name="data_augmentation"
)

print("‚úÖ Step-4 completed: Data augmentation layer created")

import matplotlib.pyplot as plt

for images, labels in train_ds.take(1):
    augmented_images = data_augmentation(images)

    plt.figure(figsize=(8, 8))
    for i in range(4):
        plt.subplot(2, 2, i + 1)
        plt.imshow(augmented_images[i])
        plt.axis("off")

    plt.suptitle("Augmented Training Images")
    plt.show()

"""## STEP 5: CUSTOM CNN MODEL BUILDING"""

import tensorflow as tf
from tensorflow.keras import layers, models

NUM_CLASSES = 4

# ----------------------------------------
# CUSTOM CNN MODEL
# ----------------------------------------
def build_custom_cnn():
    model = models.Sequential([
        # Input Layer
        layers.Input(shape=(224, 224, 3)),

        # Data Augmentation
        data_augmentation,

        # --------------------------------
        # CONV BLOCK 1
        # --------------------------------
        layers.Conv2D(32, (3, 3), padding="same", activation="relu"),
        layers.BatchNormalization(),
        layers.MaxPooling2D(),

        # --------------------------------
        # CONV BLOCK 2
        # --------------------------------
        layers.Conv2D(64, (3, 3), padding="same", activation="relu"),
        layers.BatchNormalization(),
        layers.MaxPooling2D(),

        # --------------------------------
        # CONV BLOCK 3
        # --------------------------------
        layers.Conv2D(128, (3, 3), padding="same", activation="relu"),
        layers.BatchNormalization(),
        layers.MaxPooling2D(),

        # --------------------------------
        # CLASSIFIER
        # --------------------------------
        layers.Flatten(),
        layers.Dense(256, activation="relu"),
        layers.BatchNormalization(),
        layers.Dropout(0.5),

        layers.Dense(NUM_CLASSES, activation="softmax")
    ])

    return model

# ----------------------------------------
# BUILD MODEL
# ----------------------------------------
custom_cnn = build_custom_cnn()

# ----------------------------------------
# MODEL SUMMARY
# ----------------------------------------
custom_cnn.summary()

"""## STEP 6: MODEL COMPILATION & TRAINING (CUSTOM CNN)"""

import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint

# ----------------------------------------
# COMPILE MODEL
# ----------------------------------------
custom_cnn.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

# ----------------------------------------
# CHECKPOINT ONLY (NO EARLY STOP)
# ----------------------------------------
checkpoint = ModelCheckpoint(
    "custom_cnn_best.keras",
    monitor="val_accuracy",   # better metric
    save_best_only=True,
    verbose=1
)

# ----------------------------------------
# TRAIN MODEL
# ----------------------------------------
EPOCHS = 20

history_custom = custom_cnn.fit(
    train_ds,
    validation_data=val_ds,
    epochs=EPOCHS,
    callbacks=[checkpoint]
)

"""## STEP 7: MODEL EVALUATION & METRICS (CUSTOM CNN)"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# ----------------------------------------
# LOAD BEST MODEL
# ----------------------------------------
model = tf.keras.models.load_model("custom_cnn_best.keras")

# ----------------------------------------
# EVALUATE ON TEST DATA
# ----------------------------------------
test_loss, test_accuracy = model.evaluate(test_ds)
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

# ----------------------------------------
# GET TRUE & PREDICTED LABELS
# ----------------------------------------
y_true = []
y_pred = []

for images, labels in test_ds:
    predictions = model.predict(images)
    y_pred.extend(np.argmax(predictions, axis=1))
    y_true.extend(np.argmax(labels.numpy(), axis=1))

# ----------------------------------------
# CLASSIFICATION REPORT
# ----------------------------------------
print("\nClassification Report:")
print(classification_report(y_true, y_pred, target_names=CLASS_NAMES))

# ----------------------------------------
# CONFUSION MATRIX
# ----------------------------------------
cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(6, 6))
sns.heatmap(
    cm,
    annot=True,
    fmt="d",
    cmap="Blues",
    xticklabels=CLASS_NAMES,
    yticklabels=CLASS_NAMES
)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Custom CNN")
plt.show()

"""## STEP 8: TRANSFER LEARNING ‚Äì EfficientNetB0

### 8.1 Import Required Modules
"""

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.applications.efficientnet import preprocess_input
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

"""### STAGE 1: TRAIN CLASSIFIER HEAD ONLY
‚úÖ Build model (BASE FROZEN)
"""

from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.applications.efficientnet import preprocess_input
import tensorflow as tf

def build_efficientnet_stage1():
    base_model = EfficientNetB0(
        include_top=False,
        weights="imagenet",
        input_shape=(224, 224, 3)
    )
    base_model.trainable = False  # üîí FULLY FROZEN

    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(224, 224, 3)),
        tf.keras.layers.Lambda(preprocess_input),
        base_model,
        tf.keras.layers.GlobalAveragePooling2D(),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(256, activation="relu"),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(4, activation="softmax")
    ])
    return model

"""### ‚úÖ Compile & Train (LOW LR)"""

efficientnet_stage1 = build_efficientnet_stage1()

efficientnet_stage1.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

checkpoint1 = tf.keras.callbacks.ModelCheckpoint(
    "efficientnet_stage1.keras",
    monitor="val_accuracy",
    save_best_only=True,
    verbose=1
)

history_stage1 = efficientnet_stage1.fit(
    train_ds,
    validation_data=val_ds,
    epochs=20,
    callbacks=[checkpoint1]
)

"""### STAGE 2: FINE-TUNING
‚úÖ Unfreeze top layers safely
"""

# Identify the EfficientNetB0 base model within the sequential model.
# Based on the model definition and previous errors, EfficientNetB0 is at index 1.
base_model = efficientnet_stage1.layers[1] # Correct index for EfficientNetB0

# Explicitly ensure the EfficientNetB0 base model remains non-trainable.
# This maintains the frozen state from Stage 1, as requested.
base_model.trainable = False

print("EfficientNetB0 base model is explicitly set to non-trainable.")
print("This ensures the model's weights remain frozen, as per the request to not affect the model and accuracy values from Stage 1.")

"""### ‚úÖ Recompile with VERY LOW LR"""

efficientnet_stage1.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

checkpoint2 = tf.keras.callbacks.ModelCheckpoint(
    "transfer_learning_best.keras",
    monitor="val_accuracy",
    save_best_only=True,
    verbose=1
)

history_stage2 = efficientnet_stage1.fit(
    train_ds,
    validation_data=val_ds,
    epochs=20,
    callbacks=[checkpoint2]
)

"""## STEP 9: MODEL COMPARISON

### 9.1 Load Both Models
"""

import tensorflow as tf
import numpy as np
from sklearn.metrics import classification_report
from tensorflow.keras.applications.efficientnet import preprocess_input # Ensure preprocess_input is imported

# Load models
custom_model = tf.keras.models.load_model("custom_cnn_best.keras")

# Load efficientnet_model with custom_objects
efficientnet_model = tf.keras.models.load_model(
    "transfer_learning_best.keras",
    custom_objects={"preprocess_input": preprocess_input}
)

"""### 9.2 Evaluation Function"""

def evaluate_model(model, dataset, model_name):
    y_true, y_pred = [], []

    for images, labels in dataset:
        preds = model.predict(images)
        y_pred.extend(np.argmax(preds, axis=1))
        y_true.extend(np.argmax(labels.numpy(), axis=1))

    print(f"\nüìä {model_name} Classification Report:")
    print(classification_report(y_true, y_pred, target_names=CLASS_NAMES))

"""### 9.3 Compare Models"""

evaluate_model(custom_model, test_ds, "Custom CNN")
evaluate_model(efficientnet_model, test_ds, "EfficientNetB0")